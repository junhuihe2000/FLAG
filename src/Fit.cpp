// [[Rcpp::depends(RcppEigen)]]
#include <RcppEigen.h>
/*
// [[Rcpp::depends(RcppParallel)]]
#include <RcppParallel.h>
*/

#include <iostream>

#include "train.h"
#include "Predict.h"
#include "Spectrum.h"
#include "Utils.h"

/*
using namespace Rcpp;
using namespace Eigen;
*/


/*
//' Fit Gaussian process logistic regression with local anchor embedding kernels
//'
//' @param X Training sample, a (m, d) matrix, each row indicates one point in R^d.
//' @param Y A numeric vector with length(m), count of the positive class.
//' @param X_new Testing sample, a (n-m, d) matrix, each row indicates one point in R^d.
//' @param s An integer indicating the number of the subsampling.
//' @param r An integer, the number of the nearest neighbor points.
//' @param K An integer, the number of used eigenpairs to construct heat kernel,
//' the defaulting value is `NULL`, that is, `K=min(n,s)`.
//' @param N A numeric vector with length(m), total count.
//' @param sigma A non-negative number, the weight coefficient of ridge penalty on H,
//' the defaulting value is 1e-3.
//' @param approach A character vector, taking value in c("posterior", "marginal"),
//' decides which objective function to be optimized, defaulting value is `posterior`.
//' @param cl The cluster to make parallel computing,
//' typically generated by `parallel::makeCluster(num_workers)`.
//' The defaulting value of cl is NULL, that is, sequential computing.
//' @param models A list with four components
//' \describe{
//' \item{subsample}{the method of subsampling, the defaulting value is `kmeans`.}
//' \item{kernel}{the type of kernel to compute cross similarity matrix W, the
//' defaulting value is `lae`.}
//' \item{gl}{the kind of graph Laplacian L, the defaulting value is `rw`.}
//' \item{root}{whether to square root eigenvalues of the two steps similarity matrix W,
//' the defaulting value is `FALSE`.}
//' }
//' @param output_cov Bool, whether to output covariance, defaulting value is `FALSE`.
//'
//' @return `Y_pred` A numeric vector with length(m_new), each element indicates
//' the label in the corresponding new sample point.
//' @export
//'
//' @examples
//' X0 <- matrix(rnorm(3*3), 3, 3)
//' X1 <- matrix(rnorm(3*3, 5), 3, 3)
//' Y <- c(1,1,1,0,0,0)
//' X <- rbind(X0,X1)
//' X0_new <- matrix(rnorm(10*3),10,3)
//' X1_new <- matrix(rnorm(10*3, 5),10,3)
//' X_new <- rbind(X0_new, X1_new)
//' Y_new <- c(rep(1,10),rep(0,10))
//' s <- 6; r <- 3
//' K <- 5
//' Y_pred <- fit_lae_logit_gp_cpp(X, Y, X_new, s, r, K)
*/
// [[Rcpp::export(fit_lae_logit_gp_cpp)]]
Rcpp::List fit_lae_logit_gp_cpp(Eigen::MatrixXd X, Eigen::VectorXd Y, Eigen::MatrixXd X_new,
                                int s, int r, int K, Eigen::VectorXd N,
                                double sigma, std::string approach,
                                Rcpp::List models,
                                bool output_cov) {
  std::cout << "Local anchor embedding:" << std::endl;

  int m = X.rows(); int m_new = X_new.rows();
  int n = m + m_new;

  if(K<0) {
    K = s;
  }


  EigenPair eigenpair = heat_kernel_spectrum_cpp(X, X_new, s, r, K, models);

  Eigen::VectorXi idx = Eigen::VectorXi::LinSpaced(m, 0, m-1);

  // train model
  std::cout << "Training..." << std::endl;
  // empirical Bayes to optimize t
  ReturnValue res;
  if(approach=="posterior") {
    PostOFData postdata(eigenpair, Y, N, idx, K, sigma);
    res = train_lae_logit_gp_cpp(&postdata, approach);
  } else if(approach=="marginal") {
    MargOFData margdata(eigenpair, Y, N, idx, K, sigma);
    res = train_lae_logit_gp_cpp(&margdata, approach);
  } else {
    Rcpp::stop("This model selection approach is not supported!");
  }


  std::cout << "By " << approach << " method, optimal t = " << res.t \
              << ", the objective function is " << res.obj << std::endl;

  // test model
  std::cout << "Testing..." << std::endl;
  // construct covariance matrix
  Eigen::VectorXi idx0 = Eigen::VectorXi::LinSpaced(m, 0, m-1);
  Eigen::VectorXi idx1 = Eigen::VectorXi::LinSpaced(m_new, m, n-1);
  Eigen::MatrixXd Cvv = HK_from_spectrum_cpp(eigenpair, K, res.t, idx0, idx0);
  Cvv.diagonal().array() += sigma;
  Eigen::MatrixXd Cnv = HK_from_spectrum_cpp(eigenpair, K, res.t, idx1, idx0);

  // predict labels on new samples
  Eigen::VectorXd Y_pred = Rcpp::as<Eigen::VectorXd>(test_pgbinary_cpp(Cvv, Y, Cnv)["Y_pred"]);
  std::cout << "Over" << std::endl;

  if(output_cov) {
    Eigen::MatrixXd C(n,m);
    C.topRows(m) = Cvv;
    C.bottomRows(m_new) = Cnv;
    return Rcpp::List::create(Named("Y_pred")=Y_pred, Named("C")=C);
  } else {
    return Rcpp::List::create(Named("Y_pred")=Y_pred);
  }

}






/* RcppParallel often fails with RSpectra, so we choose sequential computing finally
// parallel square exponential kernel
struct SEKPAR : public RcppParallel::Worker {
  // input data
  const Eigen::VectorXd & Y, N;
  const Eigen::MatrixXd & U;
  const Eigen::VectorXi & idx;
  const Eigen::SparseMatrix<double, Eigen::RowMajor> & distances_sp;
  std::string gl, approach;
  bool root;
  double distances_mean, sigma;
  int K;
  // parameters
  const std::vector<double>& a2s;

  // output data
  std::vector<ReturnValue> & rvs;

  // initialize from Rcpp input and output matrixces (the RMatrix class
  // can be automatically converted to from the Rcpp matrix type)
  SEKPAR(const Eigen::VectorXd & Y, const Eigen::VectorXd & N, const Eigen::MatrixXd& U, const Eigen::VectorXi & idx, const Eigen::SparseMatrix<double, Eigen::RowMajor> & distances_sp,
      std::string& gl, std::string& approach, bool root, double distances_mean, double sigma, int K,
      const std::vector<double>& a2s, std::vector<ReturnValue> & rvs) : Y(Y), N(N), U(U), idx(idx), distances_sp(distances_sp), gl(gl),\
      approach(approach), root(root), distances_mean(distances_mean), sigma(sigma), K(K), a2s(a2s), rvs(rvs) {}

  // function call operator that work for the specified range (begin/end)
  void operator()(std::size_t begin, std::size_t end) {

    for(std::size_t i=begin;i<end;i++) {
      double a2 = a2s[i];

      Eigen::SparseMatrix<double, Eigen::RowMajor> Z = distances_sp;
      Z.coeffs() = Eigen::exp(-Z.coeffs()/(a2*distances_mean));

      if(gl=="cluster-normalized") {
        graphLaplacian_cpp(Z, gl, U.rightCols(1));
      } else {
        graphLaplacian_cpp(Z, gl);
      }

      EigenPair eigenpair = spectrum_from_Z_cpp(Z, K, root);

      // empirical Bayes to optimize t
      ReturnValue res;
      if(approach=="posterior") {
        PostOFData postdata(eigenpair, Y, N, idx, K, sigma);
        res = train_lae_logit_gp_cpp(&postdata, approach);
      } else if(approach=="marginal") {
        MargOFData margdata(eigenpair, Y, N, idx, K, sigma);
        res = train_lae_logit_gp_cpp(&margdata, approach);
      } else {
        Rcpp::stop("This model selection approach is not supported!");
      }

      rvs[i] = res;
    }
  }
};
*/


// Fit Gaussian process logistic regression with square exponential kernels
// [[Rcpp::export(fit_se_logit_gp_cpp)]]
Rcpp::List fit_se_logit_gp_cpp(Eigen::MatrixXd X, Eigen::VectorXd Y, Eigen::MatrixXd X_new,
                               int s, int r, int K, Eigen::VectorXd N,
                               double sigma, std::vector<double> a2s, std::string approach,
                               Rcpp::List models,
                               bool output_cov) {
  std::cout << "Square exponential kernel:" << std::endl;

  int m = X.rows(); int m_new = X_new.rows();
  int n = m + m_new; int d = X.cols();

  if(K<0) {
    K = s;
  }

  Eigen::MatrixXd X_all(n, d);
  X_all.topRows(m) = X;
  X_all.bottomRows(m_new) = X_new;
  Eigen::MatrixXd U = subsample_cpp(X_all, s, Rcpp::as<std::string>(models["subsample"]));
  Rcpp::List res_knn = KNN_cpp(X_all, U.leftCols(d), r, "Euclidean", true);
  const Eigen::MatrixXi& ind_knn = res_knn["ind_knn"];
  const Eigen::SparseMatrix<double, Eigen::RowMajor> & distances_sp = res_knn["distances_sp"];

  double distances_mean = distances_sp.coeffs().sum()/ (n*r);

  Eigen::VectorXi idx = Eigen::VectorXi::LinSpaced(m, 0, m-1);
  std::string gl = Rcpp::as<std::string>(models["gl"]);
  bool root = models["root"];
  Eigen::SparseMatrix<double, Eigen::RowMajor> Z = distances_sp;
  int l = a2s.size();

  /* parallel leads to stack imbalance and rstudio restarts
  std::vector<ReturnValue> rvs(l);
  SEKPAR sekpar(Y, N, U, idx, distances_sp, gl, approach, root, distances_mean, sigma,
                K, a2s, rvs);

  RcppParallel::parallelFor(0, l, sekpar);
  */

  // train model
  std::cout << "Training..." << std::endl;
  // grid search
  double best_t = 0, best_a2 = 0;
  double max_obj = -std::numeric_limits<double>::infinity();
  EigenPair best_eigenpair;
  for(int i=0;i<l;i++) {
    double a2 = a2s[i];

    Z.coeffs() = Eigen::exp(-distances_sp.coeffs()/(a2*distances_mean));

    if(gl=="cluster-normalized") {
      graphLaplacian_cpp(Z, gl, U.rightCols(1));
    } else {
      graphLaplacian_cpp(Z, gl);
    }

    EigenPair eigenpair = spectrum_from_Z_cpp(Z, K, root);

    // empirical Bayes to optimize t
    ReturnValue res;
    if(approach=="posterior") {
      PostOFData postdata(eigenpair, Y, N, idx, K, sigma);
      res = train_lae_logit_gp_cpp(&postdata, approach);
    } else if(approach=="marginal") {
      MargOFData margdata(eigenpair, Y, N, idx, K, sigma);
      res = train_lae_logit_gp_cpp(&margdata, approach);
    } else {
      Rcpp::stop("This model selection approach is not supported!");
    }

    if(res.obj>max_obj) {
      max_obj = res.obj;
      best_t = res.t;
      best_a2 = a2s[i];
      best_eigenpair = eigenpair;
    }
  }

  EigenPair & eigenpair = best_eigenpair;

  std::cout << "By " << approach << " method, optimal epsilon = " << std::sqrt(best_a2) << ", t = " << best_t \
            << ", the objective function is " << max_obj << std::endl;

  // test model
  std::cout << "Testing..." << std::endl;
  // construct covariance matrix
  Eigen::VectorXi idx0 = Eigen::VectorXi::LinSpaced(m, 0, m-1);
  Eigen::VectorXi idx1 = Eigen::VectorXi::LinSpaced(m_new, m, n-1);
  Eigen::MatrixXd Cvv = HK_from_spectrum_cpp(eigenpair, K, best_t, idx0, idx0);
  Cvv.diagonal().array() += sigma;
  Eigen::MatrixXd Cnv = HK_from_spectrum_cpp(eigenpair, K, best_t, idx1, idx0);

  // predict labels on new samples
  Eigen::VectorXd Y_pred = Rcpp::as<Eigen::VectorXd>(test_pgbinary_cpp(Cvv, Y, Cnv)["Y_pred"]);
  std::cout << "Over" << std::endl;

  if(output_cov) {
    Eigen::MatrixXd C(n,m);
    C.topRows(m) = Cvv;
    C.bottomRows(m_new) = Cnv;
    return Rcpp::List::create(Named("Y_pred")=Y_pred, Named("C")=C);
  } else {
    return Rcpp::List::create(Named("Y_pred")=Y_pred);
  }
}


// Fit Gaussian process logistic regression with Nystrom extension
// [[Rcpp::export(fit_nystrom_logit_gp_cpp)]]
Rcpp::List fit_nystrom_logit_gp_cpp(Eigen::MatrixXd X, Eigen::VectorXd Y, Eigen::MatrixXd X_new,
                                    int s, int K, Eigen::VectorXd N,
                                    double sigma, std::vector<double> a2s, std::string approach,
                                    Rcpp::List models,
                                    bool output_cov) {
  std::cout << "Nystrom extension:" << std::endl;

  int m = X.rows(); int m_new = X_new.rows(); int n = m + m_new;
  int d = X.cols();

  Eigen::MatrixXd X_all(n,d);
  X_all.topRows(m) = X;
  X_all.bottomRows(m_new) = X_new;

  const std::string subsample = Rcpp::as<std::string>(models["subsample"]);

  const Eigen::MatrixXd U = subsample_cpp(X_all, s, subsample).leftCols(d);

  const Eigen::MatrixXd distances_UU  = ((-2*U*U.transpose()).colwise() + U.rowwise().squaredNorm()).rowwise() + U.rowwise().squaredNorm().transpose();
  const Eigen::MatrixXd distances_XU = ((-2*X*U.transpose()).colwise() + X.rowwise().squaredNorm()).rowwise() + U.rowwise().squaredNorm().transpose();
  const Eigen::MatrixXd distances_allU = ((-2*X_all*U.transpose()).colwise() + X_all.rowwise().squaredNorm()).rowwise() + U.rowwise().squaredNorm().transpose();

  double distances_mean = distances_UU.array().sum()/(s*s);

  Eigen::VectorXi idx = Eigen::VectorXi::LinSpaced(m,0,m-1);

  // train model
  std::cout << "Training..." << std::endl;
  // grid search
  double best_t = 0, best_a2 = 0;
  double max_obj = -std::numeric_limits<double>::infinity();
  EigenPair best_eigenpair;
  Eigen::MatrixXd best_Z_UU;
  int l = a2s.size();

  Rcpp::Environment RSpectra = Rcpp::Environment::namespace_env("RSpectra");
  Rcpp::Function eigs_sym = RSpectra["eigs_sym"];

  for(int i=0;i<l;i++) {
    double a2 = a2s[i];

    Eigen::MatrixXd Z_UU = Eigen::exp(-distances_UU.array()/(a2*distances_mean));
    Eigen::VectorXd Z_UU_rowsums = Z_UU.rowwise().sum().array()+1e-5;
    Eigen::MatrixXd A_UU = (1.0/Z_UU_rowsums.array()).matrix().asDiagonal()*Z_UU*(1.0/Z_UU_rowsums.array()).matrix().asDiagonal();
    Eigen::DiagonalMatrix<double, Eigen::Dynamic> sqrt_D_inv = (1.0/(A_UU.rowwise().sum().array()+1e-5).sqrt()).matrix().asDiagonal();
    Eigen::MatrixXd W_UU = sqrt_D_inv*A_UU*sqrt_D_inv;


    Rcpp::List res_eigs = eigs_sym(Rcpp::Named("A")=W_UU,
                                   Rcpp::Named("k")=K);
    EigenPair eigenpair_UU(Rcpp::as<Eigen::VectorXd>(res_eigs["values"]),
                           Rcpp::as<Eigen::MatrixXd>(res_eigs["vectors"]));
    eigenpair_UU.vectors = std::sqrt(s)*sqrt_D_inv*eigenpair_UU.vectors;

    // Nystrom extension formula
    Eigen::MatrixXd Z_XU = Eigen::exp(-distances_XU.array()/(a2*distances_mean));
    Eigen::VectorXd Z_XU_rowsums = Z_XU.rowwise().sum().array()+1e-5;
    Eigen::MatrixXd A_XU = (1.0/Z_XU_rowsums.array()).matrix().asDiagonal()*Z_XU*(1.0/Z_UU_rowsums.array()).matrix().asDiagonal();
    Eigen::DiagonalMatrix<double, Eigen::Dynamic> D_inv = (1.0/(A_XU.rowwise().sum().array()+1e-5)).matrix().asDiagonal();
    Eigen::MatrixXd W_XU = D_inv*A_XU;
    EigenPair eigenpair = eigenpair_UU;
    eigenpair.vectors = W_XU*eigenpair.vectors*(1.0/eigenpair.values.array()).matrix().asDiagonal();

    // empirical Bayes to optimize t
    ReturnValue res;
    if(approach=="posterior") {
      PostOFData postdata(eigenpair, Y, N, idx, K, sigma);
      res = train_lae_logit_gp_cpp(&postdata, approach);
    } else if(approach=="marginal") {
      MargOFData margdata(eigenpair, Y, N, idx, K, sigma);
      res = train_lae_logit_gp_cpp(&margdata, approach);
    } else {
      Rcpp::stop("This model selection approach is not supported!");
    }

    if(res.obj>max_obj) {
      max_obj = res.obj;
      best_t = res.t;
      best_a2 = a2s[i];
      best_eigenpair = eigenpair_UU;
      best_Z_UU = Z_UU;
    }

  }

  std::cout << "By " << approach << " method, optimal epsilon = " << std::sqrt(best_a2) << ", t = " << best_t \
            << ", the objective function is " << max_obj << std::endl;

  // test model
  std::cout << "Testing..." << std::endl;
  // Nystrom extension
  Eigen::MatrixXd Z_allU = Eigen::exp(-distances_allU.array()/(best_a2*distances_mean));
  Eigen::MatrixXd A_allU = (1.0/Z_allU.rowwise().sum().array()).matrix().asDiagonal()*Z_allU*(1.0/(best_Z_UU.colwise().sum().array())).matrix().asDiagonal();
  Eigen::MatrixXd W_allU = (1.0/A_allU.rowwise().sum().array()).matrix().asDiagonal()*A_allU;
  EigenPair & eigenpair = best_eigenpair;
  eigenpair.vectors = W_allU*eigenpair.vectors*(1.0/eigenpair.values.array()).matrix().asDiagonal();
  // construct covariance matrix
  Eigen::VectorXi idx0 = Eigen::VectorXi::LinSpaced(m, 0, m-1);
  Eigen::VectorXi idx1 = Eigen::VectorXi::LinSpaced(m_new, m, n-1);
  Eigen::MatrixXd Cvv = HK_from_spectrum_cpp(eigenpair, K, best_t, idx0, idx0);
  Cvv.diagonal().array() += sigma;
  Eigen::MatrixXd Cnv = HK_from_spectrum_cpp(eigenpair, K, best_t, idx1, idx0);

  // predict labels on new samples
  Eigen::VectorXd Y_pred = Rcpp::as<Eigen::VectorXd>(test_pgbinary_cpp(Cvv, Y, Cnv)["Y_pred"]);
  std::cout << "Over" << std::endl;

  if(output_cov) {
    Eigen::MatrixXd C(n,m);
    C.topRows(m) = Cvv;
    C.bottomRows(m_new) = Cnv;
    return Rcpp::List::create(Named("Y_pred")=Y_pred, Named("C")=C);
  } else {
    return Rcpp::List::create(Named("Y_pred")=Y_pred);
  }
}



// Fit logistic regression with GLGP
// [[Rcpp::export(fit_gl_logit_gp_cpp)]]
Rcpp::List fit_gl_logit_gp_cpp(Eigen::MatrixXd X, Eigen::VectorXd Y, Eigen::MatrixXd X_new,
                                int K, Eigen::VectorXd N,
                                double sigma, std::vector<double> a2s,
                                double threshold, bool sparse,
                                std::string approach,
                                Rcpp::List models,
                                bool output_cov) {
  std::cout << "Graph Laplacian Gaussian process:" << std::endl;

  int m = X.rows(); int m_new = X_new.rows(); int n = m + m_new;
  int d = X.cols();

  Eigen::MatrixXd X_all(n,d);
  X_all.topRows(m) = X;
  X_all.bottomRows(m_new) = X_new;

  const std::string subsample = Rcpp::as<std::string>(models["subsample"]);

  const Eigen::MatrixXd distances  = ((-2.0*X_all*X_all.transpose()).colwise() + X_all.rowwise().squaredNorm()).rowwise() + X_all.rowwise().squaredNorm().transpose();
  double distances_mean = distances.array().sum()/(n*n);
  int r = std::max((int)std::round(threshold*n),3); // r will be greater than 3.
  Rcpp::List res_knn = KNN_cpp(X_all, X_all, r, "Euclidean", true);
  const Eigen::SparseMatrix<double, Eigen::RowMajor>& distances_sp = res_knn["distances_sp"];


  Eigen::VectorXi idx = Eigen::VectorXi::LinSpaced(m,0,m-1);

  // train model
  std::cout << "Training..." << std::endl;
  // grid search
  double best_t = 0, best_a2 = 0;
  double max_obj = -std::numeric_limits<double>::infinity();
  EigenPair best_eigenpair;
  int l = a2s.size();

  Rcpp::Environment RSpectra = Rcpp::Environment::namespace_env("RSpectra");
  Rcpp::Function eigs_sym = RSpectra["eigs_sym"];

  for(int i=0;i<l;i++) {
    double a2 = a2s[i];
    EigenPair eigenpair;
    if(sparse) {
      Eigen::SparseMatrix<double> Z(distances_sp);
      Z.coeffs() = Eigen::exp(-Z.coeffs()/(a2*distances_mean));
      Z = (Z + Eigen::SparseMatrix<double>(Z.transpose()))/2.0;
      Eigen::VectorXd Z_rowsum = Z*Eigen::VectorXd::Ones(n);
      Eigen::SparseMatrix<double> A = (1.0/(Z_rowsum.array()+1e-5)).matrix().asDiagonal()*Z*(1.0/(Z_rowsum.array()+1e-5)).matrix().asDiagonal();
      Eigen::VectorXd sqrt_D_inv = 1.0/((A*Eigen::VectorXd::Ones(n)).array()+1e-5).sqrt();
      Eigen::SparseMatrix<double> W = sqrt_D_inv.asDiagonal()*A*sqrt_D_inv.asDiagonal();

      Rcpp::List res_eigs = eigs_sym(Rcpp::Named("A")=W,
                                     Rcpp::Named("k")=K);
      eigenpair = EigenPair(Rcpp::as<Eigen::VectorXd>(res_eigs["values"]),
                            Rcpp::as<Eigen::MatrixXd>(res_eigs["vectors"]));
      eigenpair.vectors = std::sqrt(n)*sqrt_D_inv.asDiagonal()*eigenpair.vectors;
    } else {
      Eigen::MatrixXd Z = Eigen::exp(-distances.array()/(a2*distances_mean));

      Eigen::VectorXd Z_rowsum = Z*Eigen::VectorXd::Ones(n);
      Eigen::MatrixXd A = (1.0/(Z_rowsum.array()+1e-5)).matrix().asDiagonal()*Z*(1.0/(Z_rowsum.array()+1e-5)).matrix().asDiagonal();
      Eigen::VectorXd sqrt_D_inv = 1.0/((A*Eigen::VectorXd::Ones(n)).array()+1e-5).sqrt();
      Eigen::MatrixXd W = sqrt_D_inv.asDiagonal()*A*sqrt_D_inv.asDiagonal();

      Rcpp::List res_eigs = eigs_sym(Rcpp::Named("A")=W,
                                     Rcpp::Named("k")=K);
      eigenpair = EigenPair(Rcpp::as<Eigen::VectorXd>(res_eigs["values"]),
                            Rcpp::as<Eigen::MatrixXd>(res_eigs["vectors"]));
      eigenpair.vectors = std::sqrt(n)*sqrt_D_inv.asDiagonal()*eigenpair.vectors;
    }

    // empirical Bayes to optimize t
    ReturnValue res;
    if(approach=="posterior") {
      PostOFData postdata(eigenpair, Y, N, idx, K, sigma);
      res = train_lae_logit_gp_cpp(&postdata, approach);
    } else if(approach=="marginal") {
      MargOFData margdata(eigenpair, Y, N, idx, K, sigma);
      res = train_lae_logit_gp_cpp(&margdata, approach);
    } else {
      Rcpp::stop("This model selection approach is not supported!");
    }

    if(res.obj>max_obj) {
      max_obj = res.obj;
      best_t = res.t;
      best_a2 = a2s[i];
      best_eigenpair = eigenpair;
    }

  }

  EigenPair & eigenpair = best_eigenpair;

  std::cout << "By " << approach << " method, optimal epsilon = " << std::sqrt(best_a2) << ", t = " << best_t \
            << ", the objective function is " << max_obj << std::endl;

  // test model
  std::cout << "Testing..." << std::endl;
  // construct covariance matrix
  Eigen::VectorXi idx0 = Eigen::VectorXi::LinSpaced(m, 0, m-1);
  Eigen::VectorXi idx1 = Eigen::VectorXi::LinSpaced(m_new, m, n-1);
  Eigen::MatrixXd Cvv = HK_from_spectrum_cpp(eigenpair, K, best_t, idx0, idx0);
  Cvv.diagonal().array() += sigma;
  Eigen::MatrixXd Cnv = HK_from_spectrum_cpp(eigenpair, K, best_t, idx1, idx0);

  // predict labels on new samples
  Eigen::VectorXd Y_pred = Rcpp::as<Eigen::VectorXd>(test_pgbinary_cpp(Cvv, Y, Cnv)["Y_pred"]);
  std::cout << "Over" << std::endl;

  if(output_cov) {
    Eigen::MatrixXd C(n,m);
    C.topRows(m) = Cvv;
    C.bottomRows(m_new) = Cnv;
    return Rcpp::List::create(Named("Y_pred")=Y_pred, Named("C")=C);
  } else {
    return Rcpp::List::create(Named("Y_pred")=Y_pred);
  }

}
