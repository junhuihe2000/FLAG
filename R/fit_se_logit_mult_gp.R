#' Fit Gaussian process logistic mulotinomial regression with square exponential kernels
#'
#' @description Compose J-1 binary logistic regression to implement the multinomial regression.
#' @param X Training sample, a (m, d) matrix, each row indicates one point in R^d.
#' @param Y A numeric vector with length(m), count of the positive class.
#' @param X_new Testing sample, a (n-m, d) matrix, each row indicates one point in R^d.
#' @param J An integer, the number of classes.
#' @param s An integer indicating the number of the subsampling.
#' @param r An integer, the number of the nearest neighbor points.
#' @param K An integer, the number of used eigenpairs to construct heat kernel,
#' the defaulting value is `NULL`, that is, `K=s`.
#' @param N A numeric vector with length(m), total count.
#' @param sigma A non-negative number, the weight coefficient of ridge penalty on H,
#' the defaulting value is 1e-3.
#' @param a2s A numeric vector, the searching range for bandwidth.
#' @param approach A character vector, taking value in c("posterior", "marginal"),
#' decides which objective function to be optimized, defaulting value is `posterior`.
#' @param cl The cluster to make parallel computing,
#' typically generated by `parallel::makeCluster(num_workers)`.
#' The defaulting value of cl is NULL, that is, sequential computing.
#' @param models A list with four components
#' \describe{
#' \item{subsample}{the method of subsampling, the defaulting value is `kmeans`.}
#' \item{kernel}{the type of kernel to compute cross similarity matrix W, the
#' defaulting value is `lae`.}
#' \item{gl}{the kind of graph Laplacian L, the defaulting value is `rw`.}
#' \item{root}{whether to square root eigenvalues of the two steps similarity matrix W,
#' the defaulting value is `FALSE`.}
#' }
#'
#' @return `Y_pred` A numeric vector with length(m_new), each element indicates
#' the label in the corresponding new sample point.
#' @export
#'
#' @examples
#' X0 <- matrix(rnorm(3*3), 3, 3)
#' X1 <- matrix(rnorm(3*3, 5), 3, 3)
#' X2 <- matrix(rnorm(3*3, -5), 3, 3)
#' Y <- c(rep(0,3), rep(1,3), rep(2,3))
#' X <- rbind(X0,X1,X2)
#' X0_new <- matrix(rnorm(10*3),10,3)
#' X1_new <- matrix(rnorm(10*3, 5),10,3)
#' X2_new <- matrix(rnorm(10*3, -5),10,3)
#' X_new <- rbind(X0_new, X1_new, X2_new)
#' Y_new <- c(rep(0,10),rep(1,10),rep(2,10))
#' s <- 6; r <- 3
#' K <- 5
#' J <- 3
#' Y_pred <- fit_se_logit_mult_gp_r(X, Y, X_new, J, s, r, K)
fit_se_logit_mult_gp_r <- function(X, Y, X_new, J, s, r, K=NULL, N=NULL, sigma=1e-3, a2s=NULL,
                                  approach ="posterior", cl=NULL,
                                  models=list(subsample="kmeans",
                                              kernel="se",
                                              gl="rw",
                                              root=FALSE)) {
  cat("Square exponential kernel:\n")
  stopifnot(J>=2)
  m = nrow(X); m_new = nrow(X_new)
  n = m + m_new

  if(is.null(K)) {
    K = s
  }

  if(is.null(a2s)) {
    a2s = exp(seq(log(0.1),log(10),length.out=10))
  }

  d = ncol(X)
  U = subsample(rbind(X,X_new), s, models$subsample)
  res_knn = KNN(rbind(X,X_new), U[,1:d,drop=FALSE], r, output=TRUE, cl=cl)
  ind_knn = res_knn[[1]]
  distances_sp = res_knn[[2]]
  distance_mean = sum(distances_sp) / (n*r)
  i_sp = rep(c(1:n),each=r); j_sp = unlist(ind_knn)
  Z = distances_sp

  # initialize model
  best_model = list(a2=NA,model_list=NA, obj=-Inf, eigenpair=NULL)


  # train model
  cat("Training...\n")
  for(a2 in a2s) {
    Z[cbind(i_sp,j_sp)] = exp(-distances_sp[cbind(i_sp,j_sp)]/(a2*distance_mean))
    Z = graphLaplacian(Z, models$gl)
    eigenpair = spectrum_from_Z(Z, K, models$root)
    # empirical Bayes to optimize t
    cat("When epsilon =", sqrt(a2),": ")
    model_list = train_lae_logit_mult_gp(eigenpair, Y, K, J, sigma, N, approach)
    obj = 0
    for(j in c(1:(J-1))) {
      obj = obj + model_list[[j]]$obj
    }
    # update model parameters
    if(obj>best_model$obj) {
      best_model$a2 = a2; best_model$model_list = model_list; best_model$obj = obj
      best_model$eigenpair = eigenpair
    }
  }

  cat("\nTraining over \n\n")
  cat("The optimal epsilon =",sqrt(best_model$a2),
      ", log", approach, "=",best_model$obj,".\n")

  # test model
  cat("Testing...\n")
  eigenpair = best_model$eigenpair
  model_list = best_model$model_list
  idx = c(1:m_new)
  Y_pred = rep(J-1, m_new)
  for(j in c(0:(J-2))) {
    modelj = model_list[[j+1]]
    tj = modelj$t
    idxj = modelj$idx
    mj = length(idxj)
    Yj = modelj$Y
    # Construct heat kernel covariance
    Cvv = HK_from_spectrum(eigenpair, K, tj, idxj, idxj)
    Cvv[cbind(rep(1:mj),rep(1:mj))] = Cvv[cbind(rep(1:mj),rep(1:mj))] + sigma
    Cvv = Matrix::symmpart(Cvv)
    Cnv = HK_from_spectrum(eigenpair, K, tj, idx+m, idxj)
    # predict binary labels on new samples
    Yj_pred = test_pgbinary(as.matrix(Cvv), Yj, as.matrix(Cnv), N) # Bug occurs
    idx1 = idx[which(Yj_pred==1)]
    Y_pred[idx1] = j
    idx = idx[-idx1]
  }
  cat("Testing over\n")
  return(Y_pred)
}
