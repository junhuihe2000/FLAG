#' Fit Gaussian process regression with local anchor embedding kernels
#'
#' @param X Training sample, a (m, d) matrix, each row indicates one point in R^d.
#' @param Y A numeric vector with length(m), response vector.
#' @param X_new Testing sample, a (n-m, d) matrix, each row indicates one point in R^d.
#' @param s An integer indicating the number of the subsampling.
#' @param r An integer, the number of the nearest neighbor points.
#' @param K An integer, the number of used eigenpairs to construct heat kernel,
#' the defaulting value is `NULL`, that is, `K=s`.
#' @param approach A character vector, taking value in c("posterior", "marginal"),
#' decides which objective function to be optimized, defaulting value is `posterior`.
#' @param cl The cluster to make parallel computing,
#' typically generated by `parallel::makeCluster(num_workers)`.
#' The defaulting value of cl is NULL, that is, sequential computing.
#' @param models A list with four components
#' \describe{
#' \item{subsample}{the method of subsampling, the defaulting value is `kmeans`.}
#' \item{kernel}{the type of kernel to compute cross similarity matrix W, the
#' defaulting value is `lae`.}
#' \item{gl}{the kind of graph Laplacian L, the defaulting value is `rw`.}
#' \item{root}{whether to square root eigenvalues of the two steps similarity matrix W,
#' the defaulting value is `FALSE`.}
#' }
#'
#' @return `Y_pred` A numeric vector with length(m_new), predictive response vector.
#' @export
#'
#' @examples
#' X <- matrix(rnorm(3*3), 3, 3)
#' Y <- rowSums(X)
#' X_new <- matrix(rnorm(10*3),10,3)
#' Y_new <- rowSums(X_new)
#' s <- 5; r <- 3
#' K <- 3
#' Y_pred = fit_lae_reg_gp(X, Y, X_new, s, r, K)
fit_lae_reg_gp <- function(X, Y, X_new, s, r, K=NULL,
                             approach ="posterior", cl=NULL,
                             models=list(subsample="kmeans",
                                         kernel="lae",
                                         gl="rw",
                                         root=FALSE)) {
  m = nrow(X)
  n = m + nrow(X_new)

  if(is.null(K)) {
    K = s
  }


  eigenpair = heat_kernel_spectrum(X, X_new, s, r, K, cl, models)

  # train model
  # empirical Bayes to optimize theta
  opt = train_lae_reg_gp(eigenpair, Y, K, approach)
  theta = opt$theta
  t = theta[1]; sigma = theta[2]

  # test model
  if(FALSE) {
  # construct covariance matrix
  C = HK_from_spectrum(eigenpair, K, t, NULL, c(1:m))
  C[cbind(rep(1:m),rep(1:m))] = C[cbind(rep(1:m),rep(1:m))] + sigma^2
  Cvv = C[1:m,]
  Cnv = C[(m+1):n,]

  # predict labels on new samples
  Y_pred = test_reg(as.matrix(Cvv), Y, as.matrix(Cnv))
  }

  Y_pred = test_reg_robust(eigenpair, Y, theta, K)

  return(Y_pred)
}
